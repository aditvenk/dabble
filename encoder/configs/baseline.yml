training:
  batch_size: 128
  epochs: 2
  max_len: 256
  learning_rate: 3e-4
  weight_decay: 0.01
  use_amp: true
  use_grad_scaling: false
  warmup_steps: 1000
  use_wandb: true
  use_compile: true
  gradient_accumulation_steps: 4

model:
  max_len: 256
  d_model: 512
  n_heads: 8
  num_layers: 8
  dim_feedforward: 2048
  dropout: 0.1
  use_checkpoint: true

wandb:
  project: dabble-single-gpu-encoder
  # group: encoder
